{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "r_getter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqEHBGdTliMQyEf2DCRB4n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reckoning-machines/sec_text_nlp/blob/master/r_getter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxREunso0REA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# why use R here?  \n",
        "# edgarWebR pulls sections really well\n",
        "# do i want to find a python library for the same thing?  sure.\n",
        "# do i want to write a python utils file for the same thing?  not really but we may have to!\n",
        "\n",
        "#first pull the ticker list with google drive handler lib\n",
        "!git clone https://gist.github.com/dc7e60aa487430ea704a8cb3f2c5d6a6.git /tmp/colab_util_repo\n",
        "!mv /tmp/colab_util_repo/colab_util.py colab_util.py \n",
        "!rm -r /tmp/colab_util_repo\n",
        "from colab_util import *\n",
        "drive_handler = GoogleDriveHandler()\n",
        "\n",
        "drive_handler.download('test_ticker_list.csv', target_path='test_ticker_list.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04jdag6BhSmY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "d0384d2c-f158-498c-b76e-77829d65b571"
      },
      "source": [
        "# activate R magic\n",
        "%load_ext rpy2.ipython"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/rpy2/robjects/pandas2ri.py:14: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.\n",
            "  from pandas.core.index import Index as PandasIndex\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/robjects/pandas2ri.py:34: UserWarning: pandas >= 1.0 is not supported.\n",
            "  warnings.warn('pandas >= 1.0 is not supported.')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2x4-h6m0x-V",
        "colab_type": "code",
        "outputId": "70cce42d-b395-4d2d-bfb6-b788aeccea3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "%%R\n",
        "\n",
        "#R GETTER.  For this example, only one ticker (head,1) is pulled \n",
        "#file is saved into local \n",
        "#then python code cell pushes file to google sheet\n",
        "#devtools::install_github(\"tidyverse/googlesheets4\")\n",
        "\n",
        "#devtools::install_version(\"xml2\", version = \"1.2.2\", repos = \"http://cran.us.r-project.org\")\n",
        "#file creates a set of csv from ticker list which include metadata & text data.\n",
        "\n",
        "\n",
        "#devtools::install_packages('trinker/textclean')\n",
        "devtools::install_github(\"mwaldstein/edgarWebR\")\n",
        "devtools::install_github(\"r-lib/xml2\") #this for edgarWebR \n",
        "devtools::install_github('trinker/textclean')\n",
        "\n",
        "library(edgarWebR) #this is an up to date library with an active maintainer.\n",
        "library(xml2)\n",
        "library(knitr)\n",
        "library(dplyr)\n",
        "library(purrr)\n",
        "library(rvest)\n",
        "library(tidyr)\n",
        "library(readr)\n",
        "#library(textshape)\n",
        "#library(lexicon)\n",
        "library(textclean)\n",
        "\n",
        "#library(log4r) TODO logging file.\n",
        "#library(googlesheets4)\n",
        "#gs4_deauth()\n",
        "#tickers list\n",
        "#str_sheet <- \"1_xcDVKjR2jqE-w5LqxIWnrDYstpSrE5nFSSY0WXNOVE\"\n",
        "#df_tickers <- read_sheet(str_sheet)\n",
        "\n",
        "#helper functions\n",
        "\n",
        "get_filings_links <-function(str_ticker) {\n",
        "  df_filings <- company_filings(str_ticker, type = \"10-\", count = 20)\n",
        "  df_filings <- df_filings[df_filings$type == \"10-K\" | df_filings$type == \"10-Q\", ]\n",
        "  df_filing_infos <- map_df(df_filings$href, filing_information)\n",
        "  df_filings <- bind_cols(df_filings, df_filing_infos)\n",
        "  return(head(as_tibble(df_filings),6))\n",
        "}\n",
        "\n",
        "get_section_text <- function(str_href, str_section, str_search) {\n",
        "  \n",
        "  df_filing_documents <- filing_documents(str_href)\n",
        "  str_doc_href <- df_filing_documents[df_filing_documents$type == \"10-K\" | df_filing_documents$type == \"10-Q\",]$href\n",
        "  doc <- parse_filing(str_doc_href)\n",
        "\n",
        "  df_txt <- doc[grepl(str_section, doc$item.name, ignore.case = TRUE) & grepl(str_search, doc$item.name, ignore.case = TRUE), ] # only discussion for now\n",
        "  #we could do some text preprocessing here.\n",
        "\n",
        "  df_txt <- as_tibble(df_txt) %>%\n",
        "    mutate(text = textclean::strip(text)) %>%\n",
        "    mutate(section = str_search)\n",
        "\n",
        "  return(df_txt)\n",
        "}\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R[write to console]: Skipping install of 'edgarWebR' from a github remote, the SHA1 (e7fa70ea) has not changed since last install.\n",
            "  Use `force = TRUE` to force installation\n",
            "\n",
            "R[write to console]: Skipping install of 'xml2' from a github remote, the SHA1 (876759f3) has not changed since last install.\n",
            "  Use `force = TRUE` to force installation\n",
            "\n",
            "R[write to console]: Downloading GitHub repo trinker/textclean@master\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "✔  checking for file ‘/tmp/Rtmp7JAC23/remotes94849abe5c4/trinker-textclean-184d786/DESCRIPTION’\n",
            "─  preparing ‘textclean’:\n",
            "✔  checking DESCRIPTION meta-information\n",
            "─  checking for LF line-endings in source and make files and shell scripts\n",
            "─  checking for empty or unneeded directories\n",
            "─  looking to see if a ‘data/datalist’ file should be added\n",
            "─  building ‘textclean_0.9.5.tar.gz’\n",
            "   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "R[write to console]: Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "R[write to console]: Parsed with column specification:\n",
            "cols(\n",
            "  X1 = col_double(),\n",
            "  `GICS Sector` = col_character(),\n",
            "  Symbol = col_character(),\n",
            "  mkt_cap = col_double()\n",
            ")\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UYcX52F7_pu",
        "colab_type": "code",
        "outputId": "4a6da527-9526-4d4c-a7a6-061a93658df9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "%%R\n",
        "\n",
        "get_document_text <- function(str_ticker, force = FALSE) { #not using force yet\n",
        "  start_time <- Sys.time()\n",
        "  \n",
        "  print(str_ticker)\n",
        "  \n",
        "  str_write_name <- paste0('sec_data_folder/',str_ticker)\n",
        "  #paste0(\"~/data/\",str_ticker,\"_sec_text\")\n",
        "\n",
        "  print(\"get filings links ...\")\n",
        "\n",
        "  df_filings <- get_filings_links(str_ticker)\n",
        "\n",
        "  #print(df_filings)\n",
        "\n",
        "  print(\"get section text ...\")\n",
        "\n",
        "  df_data <- (df_filings) %>% \n",
        "    rowwise() %>%\n",
        "    mutate(nest_discussion = map(.x = href, str_section = 'item 2|item 7',str_search = 'discussion', .f = get_section_text)) %>%\n",
        "    mutate(nest_qualitative = map(.x = href, str_section = 'item 3|item 7a', str_search = 'qualitative', .f = get_section_text)) %>%\n",
        "    mutate(nest_controls = map(.x = href, str_section = 'item 4|item 9a',str_search = 'controls', .f = get_section_text)) %>%\n",
        "    mutate(nest_risk = map(.x = href, str_section = 'item 1',str_search = 'risk factors', .f = get_section_text)) %>%\n",
        "    ungroup() %>%\n",
        "    select(period_date,filing_date,type,form_name,documents,nest_discussion,nest_qualitative,nest_controls,nest_risk) %>%\n",
        "    group_by(period_date) %>%\n",
        "    arrange(desc(period_date))\n",
        "  \n",
        "  #jenky - find a rowwise application\n",
        "  a <- df_data %>% \n",
        "    select(period_date,filing_date,type,form_name,documents,nest_discussion) %>%\n",
        "    unnest(nest_discussion)\n",
        "  b <- df_data %>% \n",
        "    select(period_date,filing_date,type,form_name,documents,nest_qualitative) %>%\n",
        "    unnest(nest_qualitative)\n",
        "  c <- df_data %>% \n",
        "    select(period_date,filing_date,type,form_name,documents,nest_controls) %>%\n",
        "    unnest(nest_controls)\n",
        "  d <- df_data %>% \n",
        "    select(period_date,filing_date,type,form_name,documents,nest_risk) %>%\n",
        "    unnest(nest_risk)\n",
        "\n",
        "  print(\"write to local csv  ...\")\n",
        "\n",
        "  #ss <- gs4_create(str_write_name)\n",
        "  \n",
        "  #df_data <- rbind(a,b,c,d) %>%\n",
        "  #  googlesheets4::sheet_write(ss, sheet = \"sec_data\")\n",
        "  \n",
        "  df_data <- rbind(a,b,c,d) %>%\n",
        "    write_csv(paste0(str_write_name,\".csv\"))\n",
        "\n",
        "  end_time <- Sys.time()\n",
        "\n",
        "  print(end_time - start_time)\n",
        "\n",
        "  return(df_data)\n",
        "}\n",
        "\n",
        "#long run.\n",
        "df_tickers <- read_csv('test_ticker_list.csv')\n",
        "dir.create('sec_data_folder', showWarnings = FALSE)\n",
        "\n",
        "#file creates a set of csv from ticker list which include metadata & text data.\n",
        "df_tickers <- head(df_tickers,1)\n",
        "\n",
        "df_data <- map_df(df_tickers$Symbol, get_document_text)\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R[write to console]: Parsed with column specification:\n",
            "cols(\n",
            "  X1 = col_double(),\n",
            "  `GICS Sector` = col_character(),\n",
            "  Symbol = col_character(),\n",
            "  mkt_cap = col_double()\n",
            ")\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1] \"GOOG\"\n",
            "[1] \"get filings links ...\"\n",
            "[1] \"get section text ...\"\n",
            "[1] \"write to local csv  ...\"\n",
            "Time difference of 58.26674 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuyztVzxz7uo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4debc1ce-d291-49d7-aa01-8580b0e7e4c2"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sec_data_folder already exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4BkP1_E2F0H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "d1d8d50c-e504-4d86-8960-073c4bcb3edc"
      },
      "source": [
        "#push files to google drive\n",
        "import os.path\n",
        "from os import path\n",
        "from colab_util import *\n",
        "drive_handler = GoogleDriveHandler()\n",
        "\n",
        "sec_folder_id = drive_handler.create_folder('sec_data_folder')\n",
        "\n",
        "import pandas as pd\n",
        "df_tickers = pd.read_csv('test_ticker_list.csv')\n",
        "\n",
        "for i, row in df_tickers.iterrows():\n",
        "  str_ticker = row['Symbol']\n",
        "  \n",
        "  print('working for:'+str_ticker+\".csv ...\")\n",
        "\n",
        "  str_to_file = 'sec_data_folder'\n",
        "  str_from_file = 'sec_data_folder/'+str_ticker+'.csv'\n",
        "\n",
        "  if path.exists(str_from_file): # a rare example of error handling\n",
        "    drive_handler.upload(str_from_file, parent_path=str_to_file,overwrite = False)\n",
        "\n",
        "print(\"done!\")\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sec_data_folder already exists\n",
            "working for:GOOG.csv ...\n",
            "working for:FB.csv ...\n",
            "working for:AMZN.csv ...\n",
            "working for:HD.csv ...\n",
            "working for:WMT.csv ...\n",
            "working for:PG.csv ...\n",
            "working for:XOM.csv ...\n",
            "working for:CVX.csv ...\n",
            "working for:JPM.csv ...\n",
            "working for:BAC.csv ...\n",
            "working for:JNJ.csv ...\n",
            "working for:UNH.csv ...\n",
            "working for:LMT.csv ...\n",
            "working for:UNP.csv ...\n",
            "working for:MSFT.csv ...\n",
            "working for:AAPL.csv ...\n",
            "working for:LIN.csv ...\n",
            "working for:ECL.csv ...\n",
            "working for:AMT.csv ...\n",
            "working for:CCI.csv ...\n",
            "working for:NEE.csv ...\n",
            "working for:D.csv ...\n",
            "done!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}