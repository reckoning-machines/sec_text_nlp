{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "r_getter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7STC/Rn1sdmKM2ttoaJND",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reckoning-machines/sec_text_nlp/blob/master/r_getter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxREunso0REA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# why use R here?  \n",
        "# edgarWebR pulls sections really well\n",
        "# do i want to find a python library for the same thing?  sure.\n",
        "# do i want to write a python utils file for the same thing?  not really but we may have to!\n",
        "\n",
        "#first pull the ticker list with google drive handler lib\n",
        "!git clone https://gist.github.com/dc7e60aa487430ea704a8cb3f2c5d6a6.git /tmp/colab_util_repo\n",
        "!mv /tmp/colab_util_repo/colab_util.py colab_util.py \n",
        "!rm -r /tmp/colab_util_repo\n",
        "from colab_util import *\n",
        "drive_handler = GoogleDriveHandler()\n",
        "\n",
        "drive_handler.download('test_ticker_list.csv', target_path='test_ticker_list.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04jdag6BhSmY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "d0384d2c-f158-498c-b76e-77829d65b571"
      },
      "source": [
        "# activate R magic\n",
        "%load_ext rpy2.ipython"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/rpy2/robjects/pandas2ri.py:14: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.\n",
            "  from pandas.core.index import Index as PandasIndex\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/robjects/pandas2ri.py:34: UserWarning: pandas >= 1.0 is not supported.\n",
            "  warnings.warn('pandas >= 1.0 is not supported.')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2x4-h6m0x-V",
        "colab_type": "code",
        "outputId": "70cce42d-b395-4d2d-bfb6-b788aeccea3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "%%R\n",
        "\n",
        "#R GETTER.  For this example, only one ticker (head,1) is pulled \n",
        "#file is saved into local \n",
        "#then python code cell pushes file to google sheet\n",
        "#devtools::install_github(\"tidyverse/googlesheets4\")\n",
        "\n",
        "#devtools::install_version(\"xml2\", version = \"1.2.2\", repos = \"http://cran.us.r-project.org\")\n",
        "#file creates a set of csv from ticker list which include metadata & text data.\n",
        "\n",
        "\n",
        "#devtools::install_packages('trinker/textclean')\n",
        "devtools::install_github(\"mwaldstein/edgarWebR\")\n",
        "devtools::install_github(\"r-lib/xml2\") #this for edgarWebR \n",
        "devtools::install_github('trinker/textclean')\n",
        "\n",
        "library(edgarWebR) #this is an up to date library with an active maintainer.\n",
        "library(xml2)\n",
        "library(knitr)\n",
        "library(dplyr)\n",
        "library(purrr)\n",
        "library(rvest)\n",
        "library(tidyr)\n",
        "library(readr)\n",
        "#library(textshape)\n",
        "#library(lexicon)\n",
        "library(textclean)\n",
        "\n",
        "#library(log4r) TODO logging file.\n",
        "#library(googlesheets4)\n",
        "#gs4_deauth()\n",
        "#tickers list\n",
        "#str_sheet <- \"1_xcDVKjR2jqE-w5LqxIWnrDYstpSrE5nFSSY0WXNOVE\"\n",
        "#df_tickers <- read_sheet(str_sheet)\n",
        "\n",
        "#helper functions\n",
        "\n",
        "get_filings_links <-function(str_ticker) {\n",
        "  df_filings <- company_filings(str_ticker, type = \"10-\", count = 20)\n",
        "  df_filings <- df_filings[df_filings$type == \"10-K\" | df_filings$type == \"10-Q\", ]\n",
        "  df_filing_infos <- map_df(df_filings$href, filing_information)\n",
        "  df_filings <- bind_cols(df_filings, df_filing_infos)\n",
        "  return(head(as_tibble(df_filings),6))\n",
        "}\n",
        "\n",
        "get_section_text <- function(str_href, str_section, str_search) {\n",
        "  \n",
        "  df_filing_documents <- filing_documents(str_href)\n",
        "  str_doc_href <- df_filing_documents[df_filing_documents$type == \"10-K\" | df_filing_documents$type == \"10-Q\",]$href\n",
        "  doc <- parse_filing(str_doc_href)\n",
        "\n",
        "  df_txt <- doc[grepl(str_section, doc$item.name, ignore.case = TRUE) & grepl(str_search, doc$item.name, ignore.case = TRUE), ] # only discussion for now\n",
        "  #we could do some text preprocessing here.\n",
        "\n",
        "  df_txt <- as_tibble(df_txt) %>%\n",
        "    mutate(text = textclean::strip(text)) %>%\n",
        "    mutate(section = str_search)\n",
        "\n",
        "  return(df_txt)\n",
        "}\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R[write to console]: Skipping install of 'edgarWebR' from a github remote, the SHA1 (e7fa70ea) has not changed since last install.\n",
            "  Use `force = TRUE` to force installation\n",
            "\n",
            "R[write to console]: Skipping install of 'xml2' from a github remote, the SHA1 (876759f3) has not changed since last install.\n",
            "  Use `force = TRUE` to force installation\n",
            "\n",
            "R[write to console]: Downloading GitHub repo trinker/textclean@master\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "✔  checking for file ‘/tmp/Rtmp7JAC23/remotes94849abe5c4/trinker-textclean-184d786/DESCRIPTION’\n",
            "─  preparing ‘textclean’:\n",
            "✔  checking DESCRIPTION meta-information\n",
            "─  checking for LF line-endings in source and make files and shell scripts\n",
            "─  checking for empty or unneeded directories\n",
            "─  looking to see if a ‘data/datalist’ file should be added\n",
            "─  building ‘textclean_0.9.5.tar.gz’\n",
            "   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "R[write to console]: Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "R[write to console]: Parsed with column specification:\n",
            "cols(\n",
            "  X1 = col_double(),\n",
            "  `GICS Sector` = col_character(),\n",
            "  Symbol = col_character(),\n",
            "  mkt_cap = col_double()\n",
            ")\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UYcX52F7_pu",
        "colab_type": "code",
        "outputId": "b5cd3ea5-856f-453d-f589-68ca73ff8523",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "%%R\n",
        "\n",
        "get_document_text <- function(str_ticker, force = FALSE) { #not using force yet\n",
        "  start_time <- Sys.time()\n",
        "  \n",
        "  print(str_ticker)\n",
        "  \n",
        "  str_write_name <- str_ticker \n",
        "  #paste0(\"~/data/\",str_ticker,\"_sec_text\")\n",
        "\n",
        "  print(\"get filings links ...\")\n",
        "\n",
        "  df_filings <- get_filings_links(str_ticker)\n",
        "\n",
        "  #print(df_filings)\n",
        "\n",
        "  print(\"get section text ...\")\n",
        "\n",
        "  df_data <- (df_filings) %>% \n",
        "    rowwise() %>%\n",
        "    mutate(nest_discussion = map(.x = href, str_section = 'item 2|item 7',str_search = 'discussion', .f = get_section_text)) %>%\n",
        "    mutate(nest_qualitative = map(.x = href, str_section = 'item 3|item 7a', str_search = 'qualitative', .f = get_section_text)) %>%\n",
        "    mutate(nest_controls = map(.x = href, str_section = 'item 4|item 9a',str_search = 'controls', .f = get_section_text)) %>%\n",
        "    mutate(nest_risk = map(.x = href, str_section = 'item 1',str_search = 'risk factors', .f = get_section_text)) %>%\n",
        "    ungroup() %>%\n",
        "    select(period_date,filing_date,type,form_name,documents,nest_discussion,nest_qualitative,nest_controls,nest_risk) %>%\n",
        "    group_by(period_date) %>%\n",
        "    arrange(desc(period_date))\n",
        "  \n",
        "  #jenky - find a rowwise application\n",
        "  a <- df_data %>% \n",
        "    select(period_date,filing_date,type,form_name,documents,nest_discussion) %>%\n",
        "    unnest(nest_discussion)\n",
        "  b <- df_data %>% \n",
        "    select(period_date,filing_date,type,form_name,documents,nest_qualitative) %>%\n",
        "    unnest(nest_qualitative)\n",
        "  c <- df_data %>% \n",
        "    select(period_date,filing_date,type,form_name,documents,nest_controls) %>%\n",
        "    unnest(nest_controls)\n",
        "  d <- df_data %>% \n",
        "    select(period_date,filing_date,type,form_name,documents,nest_risk) %>%\n",
        "    unnest(nest_risk)\n",
        "\n",
        "  print(\"write to local csv  ...\")\n",
        "\n",
        "  #ss <- gs4_create(str_write_name)\n",
        "  \n",
        "  #df_data <- rbind(a,b,c,d) %>%\n",
        "  #  googlesheets4::sheet_write(ss, sheet = \"sec_data\")\n",
        "  \n",
        "  df_data <- rbind(a,b,c,d) %>%\n",
        "    write_csv(paste0(str_write_name,\".csv\"))\n",
        "\n",
        "  end_time <- Sys.time()\n",
        "\n",
        "  print(end_time - start_time)\n",
        "\n",
        "  return(df_data)\n",
        "}\n",
        "\n",
        "#long run.\n",
        "df_tickers <- read_csv('test_ticker_list.csv')\n",
        "\n",
        "#file creates a set of csv from ticker list which include metadata & text data.\n",
        "df_tickers <- head(df_tickers,1)\n",
        "\n",
        "df_data <- map_df(df_tickers$Symbol, get_document_text)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1] \"GOOG\"\n",
            "[1] \"get filings links ...\"\n",
            "[1] \"get section text ...\"\n",
            "[1] \"write to local csv  ...\"\n",
            "Time difference of 1.011289 mins\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4BkP1_E2F0H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "568638a2-88fd-4dda-a0c6-ea2c15680583"
      },
      "source": [
        "from colab_util import *\n",
        "drive_handler = GoogleDriveHandler()\n",
        "\n",
        "#test_subfolder_id = drive_handler.create_folder('sec_', parent_path='test_folder')\n",
        "#test_subfolder_id\n",
        "\n",
        "import pandas as pd\n",
        "df_tickers = pd.read_csv('test_ticker_list.csv')\n",
        "\n",
        "for i, row in df_tickers.head(1).iterrows():\n",
        "  str_ticker = row['Symbol']\n",
        "  \n",
        "  print('working for:'+str_ticker+\".csv ...\")\n",
        "\n",
        "  str_to_file = str_ticker+'.csv'\n",
        "  str_from_file = str_ticker+'.csv'\n",
        "  print(str_from_file)\n",
        "  print(str_to_file)\n",
        "  drive_handler.upload(str_from_file, str_to_file)\n",
        "\n",
        "#save single file to google sheets\n",
        "#to do: loop thru all tickers in dir.\n",
        "\n",
        "#from google.colab import auth\n",
        "#auth.authenticate_user()\n",
        "\n",
        "#import gspread\n",
        "#from oauth2client.client import GoogleCredentials\n",
        "\n",
        "#gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "#import pandas as pd\n",
        "\n",
        "#df_data = pd.read_csv('GOOG.csv')\n",
        "\n",
        "#from gspread_dataframe import set_with_dataframe\n",
        "#title = 'sec_data'\n",
        "#gc.create(title)  # if not exist\n",
        "#sheet = gc.open(title).sheet1\n",
        "#set_with_dataframe(sheet, df_data) "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "working for:GOOG.csv ...\n",
            "GOOG.csv\n",
            "GOOG.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-ca6ba6d3fc35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_from_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_to_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0mdrive_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_from_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_to_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#save single file to google sheets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/colab_util.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(self, local_file_path, parent_path, overwrite)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mparent_folder_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_folder_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_file_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/colab_util.py\u001b[0m in \u001b[0;36mpath_to_id\u001b[0;34m(self, rel_path, parent_folder_id)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_folder_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfirst\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{0} not exist'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: GOOG.csv not exist"
          ]
        }
      ]
    }
  ]
}